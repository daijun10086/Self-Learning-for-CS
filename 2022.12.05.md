# 2022.12.05
## CS224N Lecture 6 Note
A) Vanishing Gradient & Gradient Explosion Problems
Vanishing gradient and gradient explosition is very important topic in DL, since they are deeply preventing the training of deep neural networks. Breifly speaking, vaninshing gradient and gradient explosion are caused by the long time step, by the knowledge of backpropagation and chain rule, we know that the ultimate gradients which used for update the parameters are multiplied by weight parameters, and beacuse the non-linear function, like sigmoid function, it will output result in the range of (0, 1), it means that if we multiply these parameters again and again will lead the ultimate gradient to be closed to 0, and that is so-called **vanishing gradient**. And the vanishing gradient will have bad effect on training process, since it means the loss of gradient and we can not update our parameters, which means the training process stop partly.

For gradient explosion, it is a symmetrical case. It is happend because some layers' output is greater than 1, you can imagine that if a great number of numbers bigger than  '1'  are multiplied, it will cause the gradient used to update the parameter get very very very big, such updates are useless, it just acts like all be set to 0.

In cs224N Notes, it has more detials and more mathematic explanation:
